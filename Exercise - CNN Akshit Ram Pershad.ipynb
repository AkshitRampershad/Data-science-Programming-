{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - CNN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for this exercise contains rendered images of 16 different types of Lego bricks. This is an image classification task: build a model that can correctly identify lego bricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Use the LEGO folder on your computer (as part of the downloaded files). Use the **train** folder and build a model to predict the **category** of each image. Then, validate your model on the images in the **valid** folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6379 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "#Image Data Generator manipulates and \"augments\" images\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "\n",
    "# Directory Iterator reads images from a directory\n",
    "\n",
    "train_data = DirectoryIterator(\n",
    "    directory=\"LEGO/train\",\n",
    "    image_data_generator = train_datagen,\n",
    "    target_size=(16, 16),                    ###### ENTER values for XXX ########\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=100,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1555 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "valid_data = DirectoryIterator(\n",
    "    directory=\"LEGO/valid\",\n",
    "    image_data_generator = valid_datagen,\n",
    "    target_size=(16, 16),                     ###### ENTER values for XXX ########\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=100,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your model \n",
    "\n",
    "**Be careful with the output layer: number of neurons must match the number of categories to predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8, 8, 16)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 32)          4640      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 3, 3, 32)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               36992     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,144\n",
      "Trainable params: 44,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu', \n",
    "                 input_shape=(16,16,3)))\n",
    "\n",
    "#model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "\n",
    "#model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "#### FIXED!!!!!!!\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# initiate adam optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 19s 272ms/step - loss: 2.0248 - accuracy: 0.2659 - val_loss: 1.2125 - val_accuracy: 0.5717\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 16s 258ms/step - loss: 1.3854 - accuracy: 0.4639 - val_loss: 0.9375 - val_accuracy: 0.6309\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 16s 255ms/step - loss: 1.2423 - accuracy: 0.5151 - val_loss: 0.9915 - val_accuracy: 0.6199\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 16s 254ms/step - loss: 1.1614 - accuracy: 0.5415 - val_loss: 0.8483 - val_accuracy: 0.6592\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 16s 255ms/step - loss: 1.1145 - accuracy: 0.5603 - val_loss: 0.8695 - val_accuracy: 0.6817\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 16s 255ms/step - loss: 1.0890 - accuracy: 0.5648 - val_loss: 0.7921 - val_accuracy: 0.6720\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 16s 254ms/step - loss: 1.0499 - accuracy: 0.5828 - val_loss: 0.7464 - val_accuracy: 0.7170\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 16s 256ms/step - loss: 1.0227 - accuracy: 0.5954 - val_loss: 0.8046 - val_accuracy: 0.6733\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 16s 257ms/step - loss: 1.0384 - accuracy: 0.5949 - val_loss: 0.7050 - val_accuracy: 0.7228\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 17s 265ms/step - loss: 1.0087 - accuracy: 0.6048 - val_loss: 0.6898 - val_accuracy: 0.7068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c601391ca0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        train_data,\n",
    "        epochs=10,\n",
    "        validation_data=valid_data,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c6086320a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "img = load_img(\"LEGO/valid/3713 Bush for Cross Axle/201706171506-0006.png\",\n",
    "               color_mode='rgb',\n",
    "               target_size=(16,16)\n",
    "              )\n",
    "\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 219ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.2342991e-03, 9.5023923e-03, 5.2290089e-10, 1.4928494e-08,\n",
       "        2.5491800e-09, 3.1572137e-02, 2.4579922e-06, 5.8376536e-05,\n",
       "        1.9879360e-01, 6.4142034e-05, 7.3034236e-05, 1.0543300e-04,\n",
       "        4.2006787e-02, 7.1340024e-01, 1.8716726e-04, 6.7747221e-09]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the image to array\n",
    "single_image = img_to_array(img)\n",
    "\n",
    "#Also divide the image values by 255 to normalize\n",
    "img_rank4 = np.expand_dims(single_image/255, axis=0)\n",
    "\n",
    "model.predict(img_rank4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.01, 0.  , 0.  , 0.  , 0.03, 0.  , 0.  , 0.2 , 0.  , 0.  ,\n",
       "        0.  , 0.04, 0.71, 0.  , 0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(img_rank4),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([13], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can predict the class directly using the following function:\n",
    "\n",
    "np.argmax(model.predict(img_rank4), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11214 Bush 3M friction with Cross axle': 0,\n",
       " '18651 Cross Axle 2M with Snap friction': 1,\n",
       " '2357 Brick corner 1x2x2': 2,\n",
       " '3003 Brick 2x2': 3,\n",
       " '3004 Brick 1x2': 4,\n",
       " '3005 Brick 1x1': 5,\n",
       " '3022 Plate 2x2': 6,\n",
       " '3023 Plate 1x2': 7,\n",
       " '3024 Plate 1x1': 8,\n",
       " '3040 Roof Tile 1x2x45deg': 9,\n",
       " '3069 Flat Tile 1x2': 10,\n",
       " '32123 half Bush': 11,\n",
       " '3673 Peg 2M': 12,\n",
       " '3713 Bush for Cross Axle': 13,\n",
       " '3794 Plate 1X2 with 1 Knob': 14,\n",
       " '6632 Technic Lever 3M': 15}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can retrieve the class labels from the train_generator:\n",
    "\n",
    "label_map = (train_data.class_indices)\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3713 Bush for Cross Axle'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can retrieve the class label of the prediction:\n",
    "\n",
    "list(label_map.keys())[np.argmax(model.predict(img_rank4), axis=-1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
